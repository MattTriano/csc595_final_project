Over the summer (for my sanity, I elect to skip the double-pace summer term), 

As an undergrad majoring in physics and mechanical engineering, I constantly had to imagine and model different physical objects. I had never taken the time to learn to draw well, so I was initially reluctant to draw out the systems I had to model and parameterize, and it was very easy to lose track of important features. When I started drawing out systems, I started becoming more productive and it was easier to follow ideas further.

Whether it's a backpacking map, a subway map, or an election map,  

I have always been fascinated by maps and their power to quickly convey information, be it information about subway lines, hiking trails, vote counts, or any other measureable feature that varies by location. Over the summer, I bought a student license to ESRIs GIS program, ArcMAP, and started working through tutorials just to see what I could do with the program. I explored some very frightening watershed shapefiles that mainly focused on the Mississippi and the gulf of Mexico. By increasing the sea level just a few feet (a conservative prediction for 2100 given current climate trends), the map immediately presented an immediate and powerful reality: much of the coast is going to flood. Just a few months later, hurricane Harvey hit Houston and validated my observations. It drove the point home; maps are an extremely power tool for both understanding reality and for communicating reality to others. Naturally, I wanted to develop an interactive map for our visualization.

After we decided on a topic, we found and cleaned some data sources. Amy and I focused more on the World Health Organization data on causes of mortality (http://www.who.int/healthinfo/statistics/mortality_rawdata/en/) while Andriy and Zach focused more on health indicators in the World Bank data. I made an iPython notebook that preprocesses the WHO data and reshapes it into a tabular form that resembles many massive, publicly available institutional datasets. I used annual popultion data from World Bank to normalize the mortality counts to country population. I wasn't sure of the best way to store and retrieve time series data and I ended up making 16 columns (named 2000 to 2015), which lead to hours of debugging where I learned about some of the nonintuitive (to me) ways that javascript implicitly typecasts things. I chose a tabular data over a JSON structure to reduce the size of the data file, thereby reducing the load time and data usage, but I haven't really worked with JS or web design before, so I don't know much about site optimization yet. 

I 

I chose ular data structure where columns contained a location-id (iso3), the country name, the mortality cause, and 16 of the columns (for 2000-2015) contained mortality data for that year.  I had considered loading this data into a JSON object, but I reasoned it would duplicate too many labels and be pretty memory inefficient. 